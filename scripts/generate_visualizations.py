#!/usr/bin/env python3
"""
Generate annotated videos with subtask labels from annotations.json.

This script reads the annotations.json file generated by generate_annotations.py
and creates annotated videos with subtask labels overlaid on each episode video.

Example usage:
    python scripts/generate_visualizations.py \
        --repo-id organization-name/dataset-name
"""

import json
import argparse
from pathlib import Path
from tqdm import tqdm
from annotator.structured import Annotation
from annotator.utils.visualization import create_annotated_video


def load_annotations(annotations_path: Path) -> dict:
    """Load annotations from JSON file."""
    if not annotations_path.exists():
        raise FileNotFoundError(
            f"\nError: {annotations_path.name} not found at: {annotations_path}\n\n"
            f"Please run generate_annotations.py and run_processors_annotations.py first."
        )
    
    with open(annotations_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get("annotations", {})


def create_annotations_from_dict(annotations_data: list[dict]) -> list[Annotation]:
    """Convert list of annotation dicts to Annotation objects."""
    annotation_list = [
        Annotation(
            name=annotation["name"],
            start_time=annotation["start_time"],
            end_time=annotation["end_time"],
            start_frame=annotation["start_frame"],
            end_frame=annotation["end_frame"]
        )
        for annotation in annotations_data
    ]
    return annotation_list


def process_episodes(dataset_dir: Path, video_filename: str = "top.mp4", annotations_file: str = "annotations_processed.json"):
    """Process all episodes from annotations file and generate annotated videos."""
    
    annotations_path = dataset_dir / annotations_file
    annotations = load_annotations(annotations_path)
    
    if not annotations:
        print(f"No annotations found in {annotations_path}")
        return
    
    episodes_dir = dataset_dir / "selected_episodes"
    
    if not episodes_dir.exists():
        raise FileNotFoundError(
            f"\nError: selected_episodes directory not found at: {episodes_dir}\n"
            f"Please run parse_dataset.py first to extract episodes."
        )
    
    print(f"\nProcessing {len(annotations)} episodes for visualization...")
    print(f"Output directory: {episodes_dir}\n")
    
    successful = 0
    skipped = 0
    failed = 0
    
    for episode_id, episode_data in tqdm(annotations.items(), desc="Generating visualizations"):
        episode_dir = episodes_dir / episode_id
        
        if not episode_dir.exists():
            tqdm.write(f"Warning: Episode directory not found: {episode_dir}, skipping")
            skipped += 1
            continue
        
        video_path = episode_dir / video_filename
        
        if not video_path.exists():
            tqdm.write(f"Warning: Video not found for {episode_id}: {video_path}, skipping")
            skipped += 1
            continue
        
        # Check if annotations exist
        annotations_data = episode_data.get("annotations", [])
        if not annotations_data:
            tqdm.write(f"Warning: No annotations found for {episode_id}, skipping")
            skipped += 1
            continue
        
        try:
            # Get fps from metadata, default to 30 if not available
            fps = episode_data.get("fps", 30)
            if fps is None:
                fps = 30
            
            # Convert annotations from dict format to Annotation objects
            annotations = create_annotations_from_dict(annotations_data)
            
            # Generate output filename (similar to test file)
            output_path = episode_dir / f"annotated_video.mp4"
            
            # Create annotated video
            create_annotated_video(
                video_path=video_path,
                annotations=annotations,
                output_path=output_path,
                fps=fps
            )
            
            successful += 1
            tqdm.write(f"✓ Generated visualization for {episode_id}: {output_path}")
            
        except Exception as e:
            failed += 1
            tqdm.write(f"Error processing {episode_id}: {e}")
            continue
    
    print(f"\n✓ Processing complete!")
    print(f"  Successful: {successful}")
    print(f"  Skipped: {skipped}")
    print(f"  Failed: {failed}")
    print(f"  Total: {len(annotations)}\n")


def main():
    parser = argparse.ArgumentParser(
        description="Generate annotated videos with subtask labels from annotations.json",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        "--repo-id",
        type=str,
        required=True,
        help="HuggingFace repository ID (e.g., organization-name/dataset-name)"
    )
    
    parser.add_argument(
        "--data-dir",
        type=str,
        default="data",
        help="Base data directory (default: data)"
    )
    
    parser.add_argument(
        "--video-filename",
        type=str,
        default="top.mp4",
        help="Name of the video file in each episode directory (default: top.mp4)"
    )
    
    parser.add_argument(
        "--annotations-file",
        type=str,
        default="annotations_processed.json",
        help="Name of the annotations file to use (default: annotations_processed.json)"
    )
    
    args = parser.parse_args()
    
    dataset_dir = Path(args.data_dir) / args.repo_id
    dataset_dir = dataset_dir.resolve()
    
    if not dataset_dir.exists():
        parser.error(
            f"Dataset directory does not exist: {dataset_dir}\n"
            f"Please run download_dataset.py and parse_dataset.py first."
        )
    
    process_episodes(
        dataset_dir=dataset_dir,
        video_filename=args.video_filename,
        annotations_file=args.annotations_file
    )


if __name__ == "__main__":
    main()


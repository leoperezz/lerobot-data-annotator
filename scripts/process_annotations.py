#!/usr/bin/env python3
"""
Process annotations and update LeRobot dataset with subtask-level task indices.

This script:
1. Clones the lerobot-dataset to an output directory
2. Loads annotations generated by generate_annotations.py
3. Updates tasks.parquet to include all unique subtasks with new task indices
4. Updates data parquet files to assign correct task_index to each frame based on subtasks

Example usage:
    python scripts/process_annotations.py \
        --repo-id organization-name/dataset-name
"""

import json
import argparse
import shutil
from pathlib import Path
from typing import Dict, List, Tuple
import pandas as pd
from tqdm import tqdm


def clone_dataset(source_dir: Path, output_dir: Path):
    """Clone the lerobot-dataset directory to output."""
    if output_dir.exists():
        print(f"Output directory already exists: {output_dir}")
        response = input("Do you want to overwrite it? (yes/no): ").strip().lower()
        if response != "yes":
            print("Aborting.")
            exit(0)
        print("Removing existing output directory...")
        shutil.rmtree(output_dir)
    
    print(f"Cloning dataset from {source_dir} to {output_dir}...")
    shutil.copytree(source_dir, output_dir)
    print("✓ Dataset cloned successfully")


def load_annotations(annotations_path: Path) -> dict:
    """Load annotations from JSON file."""
    with open(annotations_path, 'r') as f:
        data = json.load(f)
    return data.get('annotations', {})


def build_subtask_mapping(annotations: dict) -> Dict[str, int]:
    """
    Build a mapping from unique subtask names to task indices.
    
    Returns:
        Dict mapping subtask names to task indices (starting from 0)
    """
    unique_subtasks = []
    subtask_to_index = {}
    
    # Collect all unique subtask names in order of first appearance
    for episode_id in sorted(annotations.keys()):
        episode_data = annotations[episode_id]
        for annotation in episode_data['annotations']:
            subtask_name = annotation['name']
            if subtask_name not in subtask_to_index:
                subtask_to_index[subtask_name] = len(unique_subtasks)
                unique_subtasks.append(subtask_name)
    
    print(f"\nFound {len(unique_subtasks)} unique subtasks:")
    for i, subtask in enumerate(unique_subtasks):
        print(f"  {i}: {subtask}")
    print()
    
    return subtask_to_index


def update_tasks_parquet(dataset_dir: Path, subtask_mapping: Dict[str, int]):
    """
    Update tasks.parquet with all unique subtasks.
    
    Creates a new tasks.parquet with:
    - Index: subtask description
    - Column: task_index
    """
    tasks_path = dataset_dir / "meta" / "tasks.parquet"
    
    print(f"Updating {tasks_path}...")
    
    # Create DataFrame with subtask names as index and task_index as column
    tasks_df = pd.DataFrame({
        'task_index': list(subtask_mapping.values())
    }, index=list(subtask_mapping.keys()))
    
    # Save to parquet
    tasks_df.to_parquet(tasks_path)
    print(f"✓ Updated tasks.parquet with {len(subtask_mapping)} subtasks")


def load_episode_metadata(dataset_dir: Path) -> pd.DataFrame:
    """
    Load all episode metadata into a single DataFrame.
    
    Returns:
        DataFrame with episode metadata including episode_index, data/chunk_index,
        data/file_index, dataset_from_index, dataset_to_index
    """
    episodes_meta_dir = dataset_dir / "meta" / "episodes"
    meta_files = sorted(list(episodes_meta_dir.rglob("file-*.parquet")))
    
    if not meta_files:
        raise FileNotFoundError(f"No episode metadata files found in {episodes_meta_dir}")
    
    # Concatenate all metadata files
    dfs = []
    for meta_file in meta_files:
        df = pd.read_parquet(meta_file)
        dfs.append(df)
    
    episodes_df = pd.concat(dfs, ignore_index=True)
    return episodes_df


def update_data_parquets(dataset_dir: Path, 
                         annotations: dict, 
                         subtask_mapping: Dict[str, int],
                         episodes_metadata: pd.DataFrame):
    """
    Update data parquet files with correct task_index for each frame.
    
    For each episode in annotations:
    1. Find which data parquet file contains it
    2. Load the parquet file
    3. Update task_index for frames corresponding to each subtask
    4. Save the modified parquet file
    """
    print("\nUpdating data parquet files...")
    
    # Group episodes by their data file location
    files_to_update = {}  # (chunk_idx, file_idx) -> [(episode_index, episode_data), ...]
    
    for episode_id in sorted(annotations.keys()):
        episode_data = annotations[episode_id]
        episode_index = episode_data['episode_index']
        
        # Find episode metadata
        episode_meta = episodes_metadata[episodes_metadata['episode_index'] == episode_index]
        
        if len(episode_meta) == 0:
            print(f"Warning: Episode {episode_index} not found in metadata, skipping")
            continue
        
        episode_meta = episode_meta.iloc[0]
        
        chunk_idx = int(episode_meta['data/chunk_index'])
        file_idx = int(episode_meta['data/file_index'])
        dataset_from_index = int(episode_meta['dataset_from_index'])
        dataset_to_index = int(episode_meta['dataset_to_index'])
        
        key = (chunk_idx, file_idx)
        if key not in files_to_update:
            files_to_update[key] = []
        
        files_to_update[key].append({
            'episode_index': episode_index,
            'episode_data': episode_data,
            'dataset_from_index': dataset_from_index,
            'dataset_to_index': dataset_to_index
        })
    
    print(f"Need to update {len(files_to_update)} data parquet file(s)")
    
    # Process each data parquet file
    for (chunk_idx, file_idx), episodes_in_file in tqdm(files_to_update.items(), desc="Updating data files"):
        parquet_path = dataset_dir / "data" / f"chunk-{chunk_idx:03d}" / f"file-{file_idx:03d}.parquet"
        
        if not parquet_path.exists():
            print(f"Warning: Data parquet not found: {parquet_path}")
            continue
        
        # Load the parquet file
        df = pd.read_parquet(parquet_path)
        
        # Update task_index for each episode in this file
        for episode_info in episodes_in_file:
            episode_index = episode_info['episode_index']
            episode_data = episode_info['episode_data']
            dataset_from_index = episode_info['dataset_from_index']
            dataset_to_index = episode_info['dataset_to_index']
            
            # Get rows for this episode
            episode_mask = df['episode_index'] == episode_index
            episode_rows = df[episode_mask]
            
            if len(episode_rows) == 0:
                tqdm.write(f"Warning: No rows found for episode {episode_index} in {parquet_path}")
                continue
            
            # The frame_index in the parquet corresponds to the frame in the episode
            # For each subtask annotation, update the task_index
            for annotation in episode_data['annotations']:
                subtask_name = annotation['name']
                start_frame = annotation['start_frame']
                end_frame = annotation['end_frame']
                
                # Get the new task_index for this subtask
                new_task_index = subtask_mapping[subtask_name]
                
                # Update rows where frame_index is in the range [start_frame, end_frame]
                # and episode_index matches
                frame_mask = (
                    (df['episode_index'] == episode_index) &
                    (df['frame_index'] >= start_frame) &
                    (df['frame_index'] <= end_frame)
                )
                
                df.loc[frame_mask, 'task_index'] = new_task_index
        
        # Save the updated parquet file
        df.to_parquet(parquet_path, index=False)
    
    print("✓ Updated all data parquet files")


def main():
    parser = argparse.ArgumentParser(
        description="Process annotations and update LeRobot dataset with subtask-level task indices",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        "--repo-id",
        type=str,
        required=True,
        help="HuggingFace repository ID (e.g., organization-name/dataset-name)"
    )
    
    parser.add_argument(
        "--data-dir",
        type=str,
        default="data",
        help="Base data directory (default: data)"
    )
    
    parser.add_argument(
        "--output-name",
        type=str,
        default="output",
        help="Name of the output directory (default: output)"
    )
    
    args = parser.parse_args()
    
    # Setup paths
    dataset_base_dir = Path(args.data_dir) / args.repo_id
    dataset_base_dir = dataset_base_dir.resolve()
    
    if not dataset_base_dir.exists():
        parser.error(f"Dataset directory does not exist: {dataset_base_dir}")
    
    source_dataset_dir = dataset_base_dir / "lerobot-dataset"
    if not source_dataset_dir.exists():
        parser.error(f"lerobot-dataset directory not found: {source_dataset_dir}")
    
    output_dataset_dir = dataset_base_dir / args.output_name
    
    annotations_path = dataset_base_dir / "annotations.json"
    if not annotations_path.exists():
        parser.error(
            f"annotations.json not found: {annotations_path}\n"
            f"Please run generate_annotations.py first."
        )
    
    print("=" * 80)
    print("Processing Annotations")
    print("=" * 80)
    print(f"Source: {source_dataset_dir}")
    print(f"Output: {output_dataset_dir}")
    print(f"Annotations: {annotations_path}")
    print()
    
    # Step 1: Clone the dataset
    clone_dataset(source_dataset_dir, output_dataset_dir)
    
    # Step 2: Load annotations
    print("\nLoading annotations...")
    annotations = load_annotations(annotations_path)
    print(f"✓ Loaded annotations for {len(annotations)} episodes")
    
    # Step 3: Build subtask mapping
    subtask_mapping = build_subtask_mapping(annotations)
    
    # Step 4: Update tasks.parquet
    update_tasks_parquet(output_dataset_dir, subtask_mapping)
    
    # Step 5: Load episode metadata
    print("\nLoading episode metadata...")
    episodes_metadata = load_episode_metadata(output_dataset_dir)
    print(f"✓ Loaded metadata for {len(episodes_metadata)} episodes")
    
    # Step 6: Update data parquets
    update_data_parquets(output_dataset_dir, annotations, subtask_mapping, episodes_metadata)
    
    print("\n" + "=" * 80)
    print("✓ Processing complete!")
    print("=" * 80)
    print(f"\nUpdated dataset saved to: {output_dataset_dir}")
    print(f"Total subtasks: {len(subtask_mapping)}")
    print(f"Processed episodes: {len(annotations)}")
    print()


if __name__ == "__main__":
    main()

